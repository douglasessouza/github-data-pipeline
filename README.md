# ğŸ”„ GitHub Data Pipeline  
*A fully automated Python pipeline that fetches repository metadata from the GitHub API, processes the data using pandas, and uploads the results back to GitHub using the Content API.*

---

## ğŸ“Œ Overview  
This project demonstrates a real-world **Data Engineering workflow**, including:

- Extracting data from a public API (GitHub REST API v3)  
- Processing and transforming the data with Python (pandas)  
- Saving clean outputs as CSV  
- Automating upload of the processed files to another repository  
- Organizing the solution into a professional engineering folder structure  

This end-to-end pipeline can be extended to cloud workflows (AWS/GCP), job scheduling (Airflow/Prefect), or data warehouse ingestion (BigQuery/Snowflake).

---

## ğŸ—ï¸ Architecture

```markdown
## ğŸ—ï¸ Architecture

```mermaid
flowchart TD
    A[Fetch Repositories (GitHub API)] --> B[Process Data (pandas)]
    B --> C[Save CSV Files]
    C --> D[Upload CSV via GitHub Content API]
    D --> E[Target Repository (github-language-analysis)]


## **Project Structure**
github-data-pipeline/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ fetch_repos.py        # Class to extract GitHub repo metadata
â”‚   â”œâ”€â”€ transform_repos.py    # Functions to clean & process data
â”‚   â”œâ”€â”€ upload_files.py       # Class to upload CSVs via GitHub API
â”‚   â”œâ”€â”€ main_fetch.py         # Runs extraction + transformation
â”‚   â””â”€â”€ main_upload.py        # Runs file upload automation
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore


âš™ï¸ **Technologies Used**
- Python 3.10+
- pandas â€” data transformation
- requests â€” API requests
- base64 â€” encoding for GitHub content uploads
- GitHub REST API (v3)
- Virtual environment (venv) â€” dependency isolation

